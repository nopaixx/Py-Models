{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "49c21379-a1d1-4686-96f2-ee1c93c05229",
        "_uuid": "b92e734ac2c9f4ec91af8ed92703433c42294402",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation,BatchNormalization,GaussianNoise\nfrom keras.layers import Bidirectional\nfrom keras.optimizers import Adam,SGD\nfrom keras.models import Model\nfrom keras import backend as K\nfrom nltk.corpus import stopwords\nimport math\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nnp.random.seed(7)\n\ntrain_df = pd.read_table('../input/train.tsv')\ntest_df = pd.read_table('../input/test.tsv')\nprint(train_df.shape, test_df.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "4c41674a-bd59-451d-8407-440dec8c76eb",
        "_uuid": "44370631b28f23abd11dfe3b644a61b510734ff3",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def rmsle(Y, Y_pred):\n    assert Y.shape == Y_pred.shape\n    return np.sqrt(np.mean(np.square(Y_pred - Y )))\n\ndef root_mean_squared_logarithmic_error(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1)+0.0000001)\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)+0.0000001)\ndef wordCount(text):\n    try:\n        if text == 'No description yet':\n            return 0\n        else:\n            text = text.lower()\n            words = [w for w in text.split(\" \")]\n            return len(words)\n    except: \n        return 0\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\n    \ndef brandfinder(line):\n    brand = line[0]\n    name = line[1]\n    namesplit = name.split(' ')\n    if brand == 'missing':\n        for x in namesplit:\n            if x in all_brands:\n                return name\n    if name in all_brands:\n        return name\n    return brand    \n\ndef fill_missing_values(df):\n    df.category_name.fillna(value=\"missing\", inplace=True)\n    df.brand_name.fillna(value=\"missing\", inplace=True)\n    df.item_description.fillna(value=\"missing\", inplace=True)\n    df.item_description.replace('No description yet',\"missing\", inplace=True)\n    return df\ndef get_rnn_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(dataset.brand_name),\n        'category': np.array(dataset.category),\n#         'category_name': pad_sequences(dataset.seq_category, maxlen=MAX_CATEGORY_SEQ),\n        'item_condition': np.array(dataset.item_condition_id),\n        'num_vars': np.array(dataset[[\"shipping\"]]),\n        'desc_len': np.array(dataset[[\"desc_len\"]]),\n        'name_len': np.array(dataset[[\"name_len\"]]),\n        'subcat_0': np.array(dataset.subcat_0),\n        'subcat_1': np.array(dataset.subcat_1),\n        'subcat_2': np.array(dataset.subcat_2),\n    }\n    return X\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ba3b5de3-edbd-4292-a083-f79a91005a6f",
        "_uuid": "5453b487a40a81bbad222b29769445134ceda0f6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n# remove price low 3\ntrain_df = train_df.drop(train_df[(train_df.price < 3.0)].index)\ntrain_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8367e918-532b-4434-9a18-3c74a809494f",
        "_uuid": "37bc520c97edb65db4d599ec3ed21dd21f2fc65b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# get name and description lengths\n\ntrain_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\ntest_df['desc_len'] = test_df['item_description'].apply(lambda x: wordCount(x))\ntrain_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\ntest_df['name_len'] = test_df['name'].apply(lambda x: wordCount(x))\ntrain_df.head()\n\n\ntrain_df['subcat_0'], train_df['subcat_1'], train_df['subcat_2'] = \\\nzip(*train_df['category_name'].apply(lambda x: split_cat(x)))\ntest_df['subcat_0'], test_df['subcat_1'], test_df['subcat_2'] = \\\nzip(*test_df['category_name'].apply(lambda x: split_cat(x)))\n\nprint(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ddee41a9-319a-4e1c-9c11-fe89d4adc159",
        "_uuid": "fbb5a218241f57a61567d25be3cc2c1170b052db",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# try to get brand from text\nfull_set = pd.concat([train_df,test_df])\nall_brands = set(full_set['brand_name'].values)\ntrain_df.brand_name.fillna(value=\"missing\", inplace=True)\ntest_df.brand_name.fillna(value=\"missing\", inplace=True)\n\npremissing = len(train_df.loc[train_df['brand_name'] == 'missing'])\n\ntrain_df['brand_name'] = train_df[['brand_name','name']].apply(brandfinder, axis = 1)\ntest_df['brand_name'] = test_df[['brand_name','name']].apply(brandfinder, axis = 1)\n#found = premissing-len(train_df.loc[train_df['brand_name'] == 'missing'])\nprint(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b075c7fd-5bad-4d01-8892-ce83761dab97",
        "_uuid": "aff3e457bdcca63a43ccc7a3553df84168a3a2ac",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#train_df[\"price\"] = np.log1p(train_df.price)\n#print(train_df[\"price\"])\ntrain_df[\"price2\"]=(train_df.price)\ntrain_df[\"price3\"]=np.log10(train_df.price)\ntrain_df[\"price\"]=np.log1p(train_df.price)\n\n#print(train_df[\"price\"])\n# Split training examples into train/dev examples.\ntrain_df, dev_df = train_test_split(train_df, random_state=123, train_size=0.99)\n\n# Calculate number of train/dev/test examples.\nn_trains = train_df.shape[0]\nn_devs = dev_df.shape[0]\nn_tests = test_df.shape[0]\nprint(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b8370fe3-87d3-42a3-aa96-f2ecd97f852e",
        "_uuid": "c9972402aac5d2e0145d23d55f93082c8dbc11ed",
        "trusted": false
      },
      "cell_type": "code",
      "source": "full_df = pd.concat([train_df, dev_df, test_df])\n# missing values\nfull_df = fill_missing_values(full_df)\n#print(full_df.category_name[1])\nprint(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9032657f-18a2-461c-8eca-448241739512",
        "_uuid": "4a5f563b88a16f48fcfa7f1d1cec5bbb3e4bede0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#convert to categorical\nle = LabelEncoder()\nle.fit(full_df.category_name)\nfull_df['category'] = le.transform(full_df.category_name)\n\nle.fit(full_df.brand_name)\nfull_df.brand_name = le.transform(full_df.brand_name)\n\nle.fit(full_df.subcat_0)\nfull_df.subcat_0 = le.transform(full_df.subcat_0)\n\nle.fit(full_df.subcat_1)\nfull_df.subcat_1 = le.transform(full_df.subcat_1)\n\nle.fit(full_df.subcat_2)\nfull_df.subcat_2 = le.transform(full_df.subcat_2)\n\ndel le\nprint(\"done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0f3a36a5-60e7-42e1-9a62-88cd813ddb43",
        "_uuid": "24d18ba6e775cf085d04a6965f15d822e2ef3350",
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a8374c06-0ed2-40a1-ae2b-fc2dc37c8cfb",
        "_uuid": "03a8e73f4ae48de00492135d6a4b52466a20993c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#convert text description and name to sequences\nraw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower(), full_df.category_name.str.lower()])\n\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\nfull_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\nfull_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n\nfull_df['seq_name'][:5]\ndel tok_raw\nprint(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "eeb29c16-1913-43c5-ba72-03095a2c66f2",
        "_uuid": "d5f3634e2c65e47a347a11fed35bc1ab1a25f90b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n\nMAX_NAME_SEQ = 17 #17\nMAX_ITEM_DESC_SEQ = 105 #269\nMAX_CATEGORY_SEQ = 8 #8\nMAX_TEXT = np.max([\n    np.max(full_df.seq_name.max()),\n    np.max(full_df.seq_item_description.max()),\n]) + 100\nMAX_CATEGORY = np.max(full_df.category.max()) + 1\nMAX_BRAND = np.max(full_df.brand_name.max()) + 1\nMAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1\nMAX_DESC_LEN = np.max(full_df.desc_len.max()) + 1\nMAX_NAME_LEN = np.max(full_df.name_len.max()) + 1\nMAX_SUBCAT_0 = np.max(full_df.subcat_0.max()) + 1\nMAX_SUBCAT_1 = np.max(full_df.subcat_1.max()) + 1\nMAX_SUBCAT_2 = np.max(full_df.subcat_2.max()) + 1\n\ntrain = full_df[:n_trains]\ndev = full_df[n_trains:n_trains+n_devs]\ntest = full_df[n_trains+n_devs:]\n\nX_train = get_rnn_data(train)\nY_train = train.price.values.reshape(-1, 1)\nY_train2 = train.price2.values.reshape(-1, 1)\nY_train3 = train.price3.values.reshape(-1, 1)\n#Y_train = train.price2.values.reshape(-1, 1)\n\nX_dev = get_rnn_data(dev)\nY_dev = dev.price.values.reshape(-1, 1)\nY_dev2 = dev.price2.values.reshape(-1, 1)\nY_dev3 = dev.price3.values.reshape(-1, 1)\n\nX_test = get_rnn_data(test)\nprint(\"done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "9c4c0d90-e04e-46f6-82e8-f95c4e5a1cea",
        "_uuid": "15f5a251b8efff1e97c6ee59df621091c08924b9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ec26d07c-e309-4583-a478-111976ef115f",
        "_uuid": "de9962f22ee5a14559f7d3a58b3c2c8345fb0387",
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.random.seed(7)\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nfrom keras.layers import Conv1D,Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation,BatchNormalization,GaussianNoise\ndef get_model(lr=0.001, decay=0.0):\n    # Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    desc_len = Input(shape=[1], name=\"desc_len\")\n    name_len = Input(shape=[1], name=\"name_len\")\n    subcat_0 = Input(shape=[1], name=\"subcat_0\")\n    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n    #docvec=Input(shape=,name=\"docvec\")\n    \n    # Embeddings layers (adjust outputs to help model)\n    emb_name = Embedding(MAX_TEXT, 20)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n#     emb_category_name = Embedding(MAX_TEXT, 20)(category_name)\n#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n    emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n    \n\n    # rnn layers (GRUs are faster than LSTMs and speed is important here)\n    rnn_layer1 = GRU(16) (emb_item_desc)\n    rnn_layer2 = GRU(8) (emb_name)\n#     rnn_layer3 = GRU(8) (emb_category_name)\n\n    # main layers\n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n#         , Flatten() (emb_category)\n        , Flatten() (emb_item_condition)\n        , Flatten() (emb_desc_len)\n        , Flatten() (emb_name_len)\n        , Flatten() (emb_subcat_0)\n        , Flatten() (emb_subcat_1)\n        , Flatten() (emb_subcat_2)\n        , rnn_layer1\n        , rnn_layer2\n#         , rnn_layer3\n        , num_vars\n    ])\n    #GaussianNoise\n    # (incressing the nodes or adding layers does not effect the time quite as much as the rnn layers)\n#model.add(Conv1D(64, 3, activation='relu'))\n#model.add(MaxPooling1D(3))\n#model.add(Conv1D(128, 3, activation='relu'))\n#model.add(Conv1D(128, 3, activation='relu'))\n#model.add(GlobalAveragePooling1D())\n\n    main_l = Dropout(0.1)(Dense(512,kernel_initializer='normal',activation='relu') (main_l))    \n #   main_l = (BatchNormalization()(main_l))\n    main_l = Dropout(0.1)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))    \n    main_l = Dropout(0.1)(Dense(128,kernel_initializer='normal',activation='relu') (main_l))    \n    main_l = Dropout(0.1)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))    \n  #  main_l=  (GaussianNoise(0.2)(main_l))\n\n    # the output layer.\n    output = Dense(1, activation=\"linear\") (main_l)\n    output2 = Dense(1, activation=\"linear\") (main_l)\n    output3 = Dense(1, activation=\"linear\") (main_l)\n    optimizer = Adam(lr=lr, decay=decay)\n    model = Model([name, item_desc, brand_name , item_condition, \n                   num_vars, desc_len, name_len, subcat_0, subcat_1, subcat_2], [output,output2,output3])\n    model.compile(loss =root_mean_squared_logarithmic_error, optimizer = optimizer,metrics=['mae','mse'])\n    #model.compile(loss =root_mean_squared_logarithmic_error, optimizer = optimizer,metrics=['mae','mse'])\n\n    return model\n\nmodel = get_model()\nmodel.summary()\ndel model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b7abc677-b4d1-41d8-a8d6-7abb7b49248e",
        "_uuid": "ac2b34b8aa5f39c227881b30425bf77cfb345e83",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n# Set hyper parameters for the model.\nBATCH_SIZE = 1500\nepochs = 2\n\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(len(X_train['name']) / BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.005, 0.001\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\nmodel = get_model(lr=lr_init, decay=lr_decay)\nmodel.fit(\n        X_train, [Y_train,Y_train2,Y_train3], epochs=epochs, batch_size=BATCH_SIZE,  validation_data=(X_dev, [Y_dev,Y_dev2,Y_dev3]), verbose=1,\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "72371c0d-e1aa-4534-b37f-d993f968f97b",
        "_uuid": "a3936c70beb94550f1b4bd62329e21cc614ac127",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n#Y_dev_preds_rnn3 = model.predict(X_dev, batch_size=BATCH_SIZE)\n#Y_dev_preds_rnn3[0]\n#print(\" RMSLE error:\", rmsle(Y_dev, Y_dev_preds_rnn3))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e946c43e-0abc-4b74-8837-061eba0cdb1e",
        "_uuid": "00d0f01190e2996d9c9d67bf30c2d3305b9423a8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "preds=model.predict(X_test, batch_size=BATCH_SIZE,verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4032390d-1413-4e74-b53c-fabd4f9ad251",
        "_uuid": "3ad5fdcb440d65e72810ccaccd5dfb9b0490afa3",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n\npreds1 = np.expm1(preds[0])\npreds2=preds[1]\npreds3=np.power(10,preds[2])\n\nfinal=np.array((preds1+preds2+preds3)/3)\n\nsubmission = pd.DataFrame({\n        \"test_id\": test_df.test_id,\n        \"price\": final.reshape(-1),\n})\nprint(submission)\nsubmission.to_csv(\"./submisionmed.csv\", index=False)\n\nsubmission10 = pd.DataFrame({\n        \"test_id\": test_df.test_id,\n        \"price\": preds3.reshape(-1),\n})\nprint(submission10)\nsubmission10.to_csv(\"./submision10.csv\", index=False)\n\nprint(\"done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "77746e96-f892-484d-8eb1-0c6eac7c6f0b",
        "_uuid": "5f0c37dd66850d1840cc3ca2b27efcfacbe4fd8e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}