{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import h2o\nh2o.init()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c16a84f16ac8b92fec2c11147c852c8b0d30e586"
      },
      "cell_type": "code",
      "source": "def add_time_features(df):\n    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df['day'] = df['date'].apply(lambda x: x.day)\n    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n    \n    return df\ndef load_df(csv_path='../input/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\n    df = pd.read_csv(csv_path, dtype={'fullVisitorId': 'str'}, nrows=nrows)\n\n    for column in JSON_COLUMNS:\n        df = df.join(pd.DataFrame(df.pop(column).apply(pd.io.json.loads).values.tolist(), index=df.index))\n\n    return df\ntrain = load_df(\"../input/train.csv\")\ntest = load_df(\"../input/test.csv\")\ntrain.head()\n\ntrain = add_time_features(train)\ntest = add_time_features(test)\n# Convert target feature to 'float' type.\ntrain[\"transactionRevenue\"] = train[\"transactionRevenue\"].astype('float')\ntime_agg = train.groupby('date')['transactionRevenue'].agg(['count', 'sum'])\nyear_agg = train.groupby('year')['transactionRevenue'].agg(['sum'])\nmonth_agg = train.groupby('month')['transactionRevenue'].agg(['sum'])\nday_agg = train.groupby('day')['transactionRevenue'].agg(['sum'])\nweekday_agg = train.groupby('weekday')['transactionRevenue'].agg(['count','sum'])\n\n# Drop stange 'dict' column\ntrain = train.drop(['adwordsClickInfo'], axis=1)\ntest = test.drop(['adwordsClickInfo'], axis=1)\n# Drop column that exists only in train data\ntrain = train.drop(['campaignCode'], axis=1)\n# Input missing transactionRevenue values\ntrain[\"transactionRevenue\"].fillna(0, inplace=True)\n\ntest_ids = test[\"fullVisitorId\"].values\n#browser operatingSystem source fullVisitorId\n#browser operatingSystem source sessionId fullVisitorId browserSize browserVersion fullVisitorId isTrueDirect keyword operatingSystemVersion\n# Unwanted columns \nunwanted_columns = [  'visitId', 'visitStartTime', \n                     'flashVersion', \n                    'mobileDeviceInfo', 'mobileDeviceMarketingName', 'mobileDeviceModel', \n                    'mobileInputSelector', 'screenColors', \n                    'metro','networkDomain', 'networkLocation', 'adContent', 'campaign', \n                    'referralPath',\n                    'day','year']\n\ntrain = train.drop(unwanted_columns, axis=1)\ntest = test.drop(unwanted_columns, axis=1)\n# Constant columns\nconstant_columns = [c for c in train.columns if train[c].nunique()<=1]\nprint('Columns with constant values: ', constant_columns)\ntrain = train.drop(constant_columns, axis=1)\ntest = test.drop(constant_columns, axis=1)\n# Columns with more than 50% null data\nhigh_null_columns = [c for c in train.columns if train[c].count()<=len(train) * 0.5]\nprint('Columns more than 50% null values: ', high_null_columns)\ntrain = train.drop(high_null_columns, axis=1)\ntest = test.drop(high_null_columns, axis=1)\n\nprint('TRAIN SET')\nprint('Rows: %s' % train.shape[0])\nprint('Columns: %s' % train.shape[1])\nprint('Features: %s' % train.columns.values)\nprint()\nprint('TEST SET')\nprint('Rows: %s' % test.shape[0])\nprint('Columns: %s' % test.shape[1])\nprint('Features: %s' % test.columns.values)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92defdd27bb60dd3c4841edbdcc0c258db3f615c"
      },
      "cell_type": "code",
      "source": "print(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "faa15e407f964b5b3e061ac0806c0d69be1e5a75"
      },
      "cell_type": "code",
      "source": "train['sessionId'].replace(regex=True,inplace=True,to_replace=r'_',value=r'')\ntest['sessionId'].replace(regex=True,inplace=True,to_replace=r'_',value=r'')\ntrain['sessionId']\ntest['sessionId']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd61671437a8d9af5ec86f7f0dd5599cdb8430ce"
      },
      "cell_type": "code",
      "source": "\nhtrain=h2o.H2OFrame(train)\nhtest=h2o.H2OFrame(test)\nhtrain.summary()\ndel train\ndel test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8817c877830e0400830e878339025c6a06f5d081"
      },
      "cell_type": "code",
      "source": "#\nx = htrain.columns\ny = \"transactionRevenue\"\nx.remove(y)\nx.remove(\"date\")\nprint(\"done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b5d14dbb296afc835e16c45205570aefad7bc2c"
      },
      "cell_type": "code",
      "source": "htrain.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5feb62d6261f141c0c57eb797451a0fc8aa13d15"
      },
      "cell_type": "code",
      "source": "\n# For binary classification, response should be a factor\n#categoricalvars= \"deviceCategory\", \"isMobile\", \"continent\", \"month\", \"weekday\"\nhtrain[\"month\"] = htrain[\"month\"].asfactor()\nhtrain[\"weekday\"] = htrain[\"weekday\"].asfactor()\n\n\nhtest[\"month\"] = htest[\"month\"].asfactor()\nhtest[\"weekday\"] = htest[\"weekday\"].asfactor()\n#htrain[\"continent\"] = htrain[\"continent\"].asfactor()\n#htrain[\"month\"] = htrain[\"month\"].asfactor()\n#htrain[\"weekday\"] = htrain[\"weekday\"].asfactor()\n#test[y] = test[y].asfactor()\n\nhtrain.describe()\n#test[,y] <- as.factor(test[,y])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46680b80a5829f4f295ffe9d0916faa5f9e156f3"
      },
      "cell_type": "code",
      "source": "lentrain=len(htrain)\ntransactionRevenue=htrain[\"transactionRevenue\"]\nhtrain=htrain.drop(y)\nhtrain=htrain.drop('date')\nhtest=htest.drop('date')\n\n\nhtrain.describe()\nhtest.describe()\n\nhtotals=htrain.rbind(htest)\nhtotals.describe()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fae49dc764f90cc25a9b66597dae13319ea5a11a"
      },
      "cell_type": "code",
      "source": "\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "989effc78cc72fed8e2c99bacb2925499b9b9f58"
      },
      "cell_type": "code",
      "source": "#fullVisitorId+sessionId in autoencode 50\nfrom h2o.estimators.deeplearning import H2OAutoEncoderEstimator, H2ODeepLearningEstimator\n\n\ndef runmodelae(data,numouts):\n    if numouts<3:\n        numouts=5\n    if numouts>25:\n        numouts=25\n    labels = data.columns\n    print(labels)\n    ae_model = H2OAutoEncoderEstimator(activation=\"Tanh\",\n                                       hidden=[numouts],\n                                       model_id=labels,\n                                       epochs=3,                                      \n                                      # mini_batch_size=256, \n                                       ignore_const_cols=True,\n                                     #  reproducible=True,\n                                       categorical_encoding='one_hot_internal',\n                                       seed=1)\n    ae_model.train(labels, training_frame=data)  \n    \n    return ae_model\n\nallmodels=[]\n#fullVisitorId+sessionId in autoencode 50\n#fin=htotals[\"fullVisitorId\"].cbind(htotals[\"sessionId\"])\n#allmodels.append(runmodelae(fin,150))\n\n#channelGrouping medium source\nfin=htotals[\"channelGrouping\"].cbind(htotals[\"medium\"])\nfin=fin.cbind(htotals[\"source\"])\nfin=fin.cbind(htotals[\"fullVisitorId\"])\nallmodels.append(runmodelae(fin,150))\n\n#browser deviceCategory isMobile operatingSystem\nfin=htotals[\"browser\"].cbind(htotals[\"deviceCategory\"])\nfin=fin.cbind(htotals[\"isMobile\"])\nfin=fin.cbind(htotals[\"operatingSystem\"])\nfin=fin.cbind(htotals[\"sessionId\"])\nfin=fin.cbind(htotals[\"channelGrouping\"])\nallmodels.append(runmodelae(fin,150))\n\n\n#pageviews hits visitNumber\nfin=htotals[\"pageviews\"].cbind(htotals[\"hits\"])\nfin=fin.cbind(htotals[\"visitNumber\"])\nfin=fin.cbind(htotals[\"fullVisitorId\"])\nfin=fin.cbind(htotals[\"sessionId\"])\nfin=fin.cbind(htotals[\"channelGrouping\"])\nallmodels.append(runmodelae(fin,150))\n\n\n#city continent region country subContinent\nfin=htotals[\"city\"].cbind(htotals[\"continent\"])\nfin=fin.cbind(htotals[\"region\"])\nfin=fin.cbind(htotals[\"country\"])\nfin=fin.cbind(htotals[\"sessionId\"])\nfin=fin.cbind(htotals[\"subContinent\"])\nallmodels.append(runmodelae(fin,150))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdf2427e47654622954d3f32dea2911d5711c472"
      },
      "cell_type": "code",
      "source": "#channelGrouping medium source\nfin=htrain[\"channelGrouping\"].cbind(htrain[\"medium\"])\nfin=fin.cbind(htrain[\"source\"])\nfin=fin.cbind(htrain[\"fullVisitorId\"])\nf1=allmodels[0].deepfeatures(fin,0)\nf1\n\n#browser deviceCategory isMobile operatingSystem\nfin=htrain[\"browser\"].cbind(htrain[\"deviceCategory\"])\nfin=fin.cbind(htrain[\"isMobile\"])\nfin=fin.cbind(htrain[\"operatingSystem\"])\nfin=fin.cbind(htrain[\"sessionId\"])\nfin=fin.cbind(htrain[\"channelGrouping\"])\nf2=allmodels[1].deepfeatures(fin,0)\n\n\n#pageviews hits visitNumber\nfin=htrain[\"pageviews\"].cbind(htrain[\"hits\"])\nfin=fin.cbind(htrain[\"visitNumber\"])\nfin=fin.cbind(htrain[\"fullVisitorId\"])\nfin=fin.cbind(htrain[\"sessionId\"])\nfin=fin.cbind(htrain[\"channelGrouping\"])\nf3=allmodels[2].deepfeatures(fin,0)\n\n\n#city continent region country subContinent\nfin=htrain[\"city\"].cbind(htrain[\"continent\"])\nfin=fin.cbind(htrain[\"region\"])\nfin=fin.cbind(htrain[\"country\"])\nfin=fin.cbind(htrain[\"sessionId\"])\nfin=fin.cbind(htrain[\"subContinent\"])\nf4=allmodels[3].deepfeatures(fin,0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7b3980ef23d131f11749767509cafde9c0cf891"
      },
      "cell_type": "code",
      "source": "f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "448f501b02748961104cfdfc89a17c1dd074cbcf"
      },
      "cell_type": "code",
      "source": "f1\nallframes=[]\nallframes.append(f1.as_data_frame())\nallframes.append(f2.as_data_frame())\nallframes.append(f3.as_data_frame())\nallframes.append(f4.as_data_frame())\n\n\nt3=pd.concat(allframes, axis=1, sort=False)\nt3.columns=[str(x) for x in (range(0,len(t3.columns)))]\nprint(\"done\")\nprint(t3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63c756a8256708ab1a2dafc38cefbf91ce1b21fe"
      },
      "cell_type": "code",
      "source": "t3.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35a5c18402f88a87faf0cee9833972e0c72a6014"
      },
      "cell_type": "code",
      "source": "import gc\n\nallframes.clear()  # Added in python3.(? maybe 5?)\ndel allframes[:]\n\ngc.collect()\n#h2o.shutdown()\n#h2o.init()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60306b059a16c138c78b9732cedfe10f72031884"
      },
      "cell_type": "code",
      "source": "\nf=h2o.H2OFrame(t3)\nf.summary()\ndel t3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab7de7f611c9b86cd78294407be292e9bed060a4"
      },
      "cell_type": "code",
      "source": "\nxx = f.columns\ntransactionRevenue=transactionRevenue.log1p()\ntrain_supervised_features=f.cbind(transactionRevenue)\ntrain_supervised_features.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4af26c2bee2dc8546146698c8ceebea1044767f3"
      },
      "cell_type": "code",
      "source": "#htrain[\"transactionRevenue\"]\n#htrain.describe()\ny = \"log1p(transactionRevenue)\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a52452601e462fe35b6215fd5408b9edef5fc03f"
      },
      "cell_type": "code",
      "source": "train, valid = train_supervised_features.split_frame(\n    ratios=[0.9],\n    seed=1234, \n    destination_frames=['train.hex','valid.hex']\n)\nprint(\"done\")\nprint(len(train))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19a8466efb905a56629b8066279e9f77807e3db1"
      },
      "cell_type": "code",
      "source": "# Build model\n#from h2o.estimators.deeplearning import H2ODeepWaterEstimator\n#from h2o.estimators.deepwater import H2ODeepWaterEstimator\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator\n#Huber \n#quantile \n#quantile_alpha=0.8\n#distribution=\"Gaussian\"\n#, mini_batch_size=2560\nmodel = H2ODeepLearningEstimator(epochs=5, activation=\"RectifierWithDropout\", hidden=[200,100,50,25], ignore_const_cols=True, input_dropout_ratio=0.0, \n                                hidden_dropout_ratios=[0.5,0.5,0.5,0.5],nfolds=0,ignored_columns=[\"date\"],\n                                stopping_rounds=5,  stopping_tolerance=0.0001,stopping_metric=\"rmse\",\n                                 #score_interval=2, score_duty_cycle=0.5,score_training_samples=1000,score_validation_samples=1000,\n                                  #  fold_assignment=\"Modulo\",\n                                   # keep_cross_validation_predictions=True,\n                                    seed=1,standardize=True\n                                )\n    #categorical_encoding=one_hot_internal\nmodel.train(x=xx, y=y, training_frame=train,validation_frame = valid)\nmodel.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d65a118bc2567fa030d6e411a2157f7bb2e4b22d"
      },
      "cell_type": "code",
      "source": "param = {\n      \"ntrees\" : 200\n    , \"distribution\":\"tweedie\" #poisson tweedie gamma gaussian  \n    ,'tree_method':\"hist\"\n    ,'grow_policy':\"lossguide\"\n    #, \"max_depth\" : 10\n    , \"learn_rate\" : 0.01\n    , \"sample_rate\" : 0.7\n    , \"col_sample_rate_per_tree\" : 0.9\n    , \"min_rows\" : 5\n    , \"score_tree_interval\": 100\n    #, \"nfolds\":5\n    #, \"fold_assignment\":\"Modulo\"\n    #, \"keep_cross_validation_predictions\":True\n    , \"seed\":1\n    #,\"categorical_encoding\":'one_hot_internal' \n    #,\"max_abs_leafnode_pred\" :0.5\n    ,\"stopping_rounds\":10\n    ,\"stopping_metric\":'rmse'\n    ,'booster':\"dart\"\n    #,\"standardize\":False\n}\n#one_hot_internal \n#max_abs_leafnode_pred \n#categorical_encoding:\n#distribution:\nfrom h2o.estimators import H2OXGBoostEstimator\nxgmodel = H2OXGBoostEstimator(**param)\nxgmodel.train(x = xx, y = y, training_frame = train, validation_frame = valid)\nxgmodel.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c2a391291906bffbf973ae5638a376571707b0d"
      },
      "cell_type": "code",
      "source": "from h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\n## Depth 10 is usually plenty of depth for most datasets, but you never know\n#hyper_params = {'max_depth' : [range(1,30,2)]}\n#hyper_params = {'max_depth' : [4,6,8,12,16,20,24,30,35,40,50,60]} ##faster for larger datasets\nhyper_params = {'max_depth' : [6,8,12,16,20,24]} ##faster for larger datasets\n# GBM hyperparameters\n\n# Train and validate a cartesian grid of GBMs\n\n#Build initial GBM Model\ngbm_grid = H2OGradientBoostingEstimator(\n        distribution='Gaussian',\n        ## more trees is better if the learning rate is small enough \n        ## here, use \"more than enough\" trees - we have early stopping\n        ntrees=3000,\n        ## smaller learning rate is better\n        ## since we have learning_rate_annealing, we can afford to start with a \n        #bigger learning rate\n        learn_rate=0.01,\n        ## learning rate annealing: learning_rate shrinks by 1% after every tree \n        ## (use 1.00 to disable, but then lower the learning_rate)\n        learn_rate_annealing = 0.99,\n        ## sample 80% of rows per tree\n        sample_rate = 0.8,\n        ## sample 80% of columns per split\n        col_sample_rate = 0.8,\n        ## fix a random number generator seed for reproducibility\n        seed = 1234,\n        ## score every 10 trees to make early stopping reproducible \n        #(it depends on the scoring interval)\n        score_tree_interval = 10, \n        ## early stopping once the validation AUC doesn't improve by at least 0.01% for \n        #5 consecutive scoring events\n        #categorical_encoding='one_hot_internal',\n        stopping_rounds = 3,\n        stopping_metric = \"RMSE\",\n        stopping_tolerance = 1e-4)\n\n#Build grid search with previously made GBM and hyper parameters\ngrid = H2OGridSearch(model=gbm_grid,\n                     grid_id='depth_grid',\n                     hyper_params=hyper_params,\n                    search_criteria = {'strategy': \"Cartesian\"})\n\n#search_criteria = {'strategy': \"Cartesian\"}\n\n#Train grid search\ngrid.train(x=xx, \n           y=y,\n           training_frame = train,\n           validation_frame = valid)\n\n\nprint (grid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0749821eeab73f655ceeb21b3aa8c5d36926d4cb"
      },
      "cell_type": "code",
      "source": "## sort the grid models by decreasing MSE\nsorted_grid = grid.get_grid(sort_by='RMSE',decreasing=False)\nprint(sorted_grid)\nmax_depths = sorted_grid.sorted_metric_table()['max_depth'][0:3]\nnew_max = int(max(max_depths, key=int))\nnew_min = int(min(max_depths, key=int))\n\nprint (\"MaxDepth\", new_max)\nprint (\"MinDepth\", new_min)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae649c4e86d1d81aed41214c3d0b8ef57f5a5f5e"
      },
      "cell_type": "code",
      "source": "import math\n# create hyperameter and search criteria lists (ranges are inclusive..exclusive))\nhyper_params_tune = {'max_depth' : list(range(new_min,new_max+1,1)),\n                'sample_rate': [x/100. for x in range(20,101)],\n                'col_sample_rate' : [x/100. for x in range(20,101)],\n                'col_sample_rate_per_tree': [x/100. for x in range(20,101)],\n                'col_sample_rate_change_per_level': [x/100. for x in range(90,111)],\n                'min_rows': [2**x for x in range(0,int(math.log(train.nrow,2)-1)+1)],\n                'nbins': [2**x for x in range(4,11)],\n                'nbins_cats': [2**x for x in range(4,13)],\n                'min_split_improvement': [0,1e-8,1e-6,1e-4],\n                'histogram_type': [\"UniformAdaptive\",\"QuantilesGlobal\",\"RoundRobin\"]}\n\nsearch_criteria_tune = {'strategy': \"RandomDiscrete\",\n                   'max_runtime_secs': 3600,  ## limit the runtime to 60 minutes\n                   'max_models': 100,  ## build no more than 100 models\n                   'seed' : 1234,\n                   'stopping_rounds' : 3,\n                   'stopping_metric' : \"RMSE\",\n                   'stopping_tolerance': 1e-3\n                   }\n\ngbm_final_grid = H2OGradientBoostingEstimator(\n                    #distribution='gaussian',\n                    distribution='Gaussian',\n                    ## more trees is better if the learning rate is small enough \n                    ## here, use \"more than enough\" trees - we have early stopping\n                    ntrees=3000,\n                    ## smaller learning rate is better\n                    ## since we have learning_rate_annealing, we can afford to start with a \n                    #bigger learning rate\n                    learn_rate=0.01,\n                    ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                    ## (use 1.00 to disable, but then lower the learning_rate)\n                    learn_rate_annealing = 0.99,\n                    ## score every 10 trees to make early stopping reproducible \n                    #(it depends on the scoring interval)\n                    seed=1234,\n                    score_tree_interval = 10,\n                    ## fix a random number generator seed for reproducibility                                       \n                    ## early stopping once the validation AUC doesn't improve by at least 0.01% for \n                    #5 consecutive scoring events\n                    stopping_rounds = 3,\n                    #categorical_encoding='one_hot_internal',\n                    stopping_metric = \"RMSE\",\n                    stopping_tolerance = 1e-3)\n            \n#Build grid search with previously made GBM and hyper parameters\nfinal_grid = H2OGridSearch(gbm_final_grid, hyper_params = hyper_params_tune,\n                                    grid_id = 'final_grid',\n                                    search_criteria = search_criteria_tune)\n#Train grid search\nfinal_grid.train(x=xx, \n           y=y,\n           ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)\n           max_runtime_secs = 3600, \n           training_frame = train,\n           validation_frame = valid)\n\nprint (final_grid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a23db6130c35b834cd56c0298d9b34b9a649a922"
      },
      "cell_type": "code",
      "source": "sorted_final_grid = final_grid.get_grid(sort_by='RMSE',decreasing=False)\nprint (sorted_final_grid)\n\n#Get the best model from the list (the model name listed at the top of the table)\nbest_model = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][0])\nperformance_best_model = best_model.model_performance(test)\nprint (performance_best_model.rmse())\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f9f40cf368d88a8da73f6d2eb078af36098eac3"
      },
      "cell_type": "code",
      "source": "print(best_model.params)\nparams_list = []\nfor key, value in best_model.params.items():\n    params_list.append(str(key)+\" = \"+str(value['actual']))\nprint(params_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16aaa2f2121b00ac729e75690296dc2017a70d75"
      },
      "cell_type": "code",
      "source": "\n\ngbm = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][0])\n#get the parameters from the Random grid search model and modify them slightly\n#params = gbm.params\n#new_params = {\"nfolds\":5, \"model_id\":None}\n#for key in new_params.keys():\n#    params[key]['actual'] = new_params[key] \n    \n#gbm_best = H2OGradientBoostingEstimator()\n#for key in params.keys():\n#    if key in dir(gbm_best) and getattr(gbm_best,key) != params[key]['actual']:\n#        if(key=='training_frame' or key=='validation_frame'  ):\n#            1+1\n#        else:\n#            #print(params[key]+\"  \"+params[key]['actual'])\n#            setattr(gbm_best,key,params[key]['actual'])\n            \n        \nsetattr(gbm,'nfolds',5)\nsetattr(gbm,'stopping_rounds',50)\nsetattr(gbm,'fold_assignment',\"Modulo\")\nsetattr(gbm,'keep_cross_validation_predictions',True)\nsetattr(gbm,'seed',1)\ngbm.train(x=xx, y=y, training_frame=train,validation_frame=valid)\n\n#gbm = h2o.get_model(gbm_best)        \n#gbm=gbm_best\nprint (gbm.cross_validation_metrics_summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db67b1a40ea785cc2bd7d86c132d02b04aba335b"
      },
      "cell_type": "code",
      "source": "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n\n# Train a stacked ensemble using the GBM and GLM above\nensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_binomial3\",\n                                       base_models=[model,gbm,xgmodel])\nensemble.train(x=xx, y=y, training_frame=train,validation_frame=valid)\nensemble.show()\n# Eval ensemble performance on the test data\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90fd7eab8edcee86abcf8dbcaa3ef96de69603db"
      },
      "cell_type": "code",
      "source": "perf_stack_test = ensemble.model_performance(valid)\nprint(perf_stack_test)\n\nperf_stack_test = model.model_performance(valid)\nprint(perf_stack_test)\n\nperf_stack_test = gbm.model_performance(valid)\nprint(perf_stack_test)\n\nperf_stack_test = xgmodel.model_performance(valid)\nprint(perf_stack_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17dce593c5329351d670a5cfbac17d2631720530"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5da290d6a1af1748e21dc50018465215ffc4a13"
      },
      "cell_type": "code",
      "source": "train_supervised_features = ae_model.deepfeatures(htest, 1)\ntrain_supervised_features.summary()\n\npredictions = ensemble.predict(train_supervised_features)\n\n\nsubmission = pd.DataFrame({\"fullVisitorId\":test_ids})\npredictions[predictions<0] = 0\nsubmission[\"PredictedLogRevenue\"] = predictions\nsubmission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsubmission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"]\nsubmission.to_csv(\"submission.csv\", index=False)\n\n\nprint(sybmission)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}